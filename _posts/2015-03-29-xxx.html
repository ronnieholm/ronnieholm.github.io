---
layout: post
title: "Informally speed testing SharePoint Online"
date: 2015-03-29 12:00 UTC
tags: [C#]
draft: true
---

- template for backlog manually created at client sites multiple times
- use tool such as to extract (and then cleanup) schema xml after point/click
- don't like starting off from the build-in once. Too much trouble
  removing stuff. Instead start from scratch and add what's needed
- keep the number of fields to an absolute minimum (subtractive vs additive)
- don't implement at RDBMS or fully fledged issue tracking system
- in organizations learning SP such list may be a good enabler
- don't fancy exporting lists because you lose track of it's config,
  and need separate documentation besides it (evolves indepedently).
  The code solution documents every tiny configuration in code.
- BDD style description where possible (add as default value?)
- Whether defect or change request doesn't matter all much. It's something
  that needs to be fixed/implemented for the solution to add business value.

<div id="post">

<p>

</p>

<h2>Setup</h2>

<p>Let's start with common code for initializing the SharePoint
context, a utility method for timing code blocks, and general skeleton
code:</p>

<pre class="prettyprint lang-cs">
// 1. Reference Microsoft.SharePoint.Client.dll and Microsoft.SharePoint.Client.Runtime.dll
//    from C:\Program Files\Common Files\Microsoft Shared\Web Server Extensions\16\ISAPI or
//    download from https://github.com/OfficeDev/PnP/tree/master/Assemblies/16
// 2. Build as 64 bit assembly to avoid OutOfMemoryExceptions

using System;
using System.Diagnostics;
using System.IO;
using System.Linq;
using System.Security;
using System.Threading;
using Microsoft.SharePoint.Client;
using SPOFile = Microsoft.SharePoint.Client.File;

namespace SharePointOnlineSpeedTest {
    class Program {       
        const int Megabyte = 1024 * 1024;
        const int Megabit = 1000 * 1000;

        static ClientContext SetupContext(Uri siteCollection) {
            var user = "rh@bugfree.onmicrosoft.com";
            var password = "password";
            var securePassword = new SecureString();
            password.ToCharArray().ToList().ForEach(securePassword.AppendChar);
            var credentials = new SharePointOnlineCredentials(user, securePassword);
            return new ClientContext(siteCollection) { Credentials = credentials };
        }

        static double Time(Action code) {
            var sw = new Stopwatch();
            sw.Start();
            code();
            sw.Stop();
            return sw.ElapsedMilliseconds / 1000.0;
        }

        static void Main(string[] args) {
            // code below goes here
        }
    }   
}
</pre>

<p>The SharePoint client side object model offers (at least) two ways
of uploading files to a document library. One way is using
the <a href="https://msdn.microsoft.com/en-us/library/microsoft.sharepoint.client.filecreationinformation.aspx">FileCreationInformation</a>
class like so:

<pre class="prettyprint lang-cs">
// take 1
using (var ctx = SetupContext(new Uri("https://bugfree.sharepoint.com/sites/test"))) {
    var library = ctx.Web.Lists.GetByTitle("speedtest");
    var newFile =
        library.RootFolder.Files.Add(
            new FileCreationInformation {
                Url = "randombits",
                Content = System.IO.File.ReadAllBytes(@"c:\users\rh\desktop\randombits")
            });
    ctx.Load(newFile);
    ctx.ExecuteQuery();
}
</pre>

<p>This approach works well for file sizes less than two megabytes,
but otherwise throws
a <a href="http://stackoverflow.com/questions/20076871/sharepoint-2013-client-object-model-file-larger-than-2-mb-exception">request
message too long exception</a>. Hence we turn to the second option,
using
the <a href="https://msdn.microsoft.com/en-us/library/ee538285.aspx">SaveBinaryDirect
method</a> and
the <a href="http://en.wikipedia.org/wiki/WebDAV">WebDAV
protocol</a>.</p>

<pre class="prettyprint lang-cs">
// take 2
var destination = new Uri(args[0]);
var sizeInMegabytes = int.Parse(args[1]);
var contentLength = sizeInMegabytes * Megabyte;
var randomizedContent = new byte[contentLength];
new Random().NextBytes(randomizedContent);

using (var ctx = SetupContext(destination)) {
    // internally the context uses HttpWebRequest to upload and download files.
    // Thus, when uploading or downloading large files, we may experience
    //   Unhandled Exception: System.Net.WebException: The operation has timed out
    // unless we change the default timeout period of 180 seconds.
    ctx.RequestTimeout = Timeout.Infinite;

    var uploadTime = Time(() => {
        using (var ms = new MemoryStream(randomizedContent)) {
            SPOFile.SaveBinaryDirect(ctx, destination.LocalPath, ms, true);
        }
    });

    var downloadTime = Time(() => {
        var fileInformation = SPOFile.OpenBinaryDirect(ctx, destination.LocalPath);
        using (var sr = new StreamReader(fileInformation.Stream)) { 
            sr.ReadToEnd(); 
        }
    });

    Console.WriteLine(
        "{0} MB uploaded in {1:0.0} seconds at {2:0.0} Mbit/s\r\n" +
        "{0} MB downloaded in {3:0.0} seconds at {4:0.0} Mbit/s",
        sizeInMegabytes, uploadTime, contentLength / uploadTime * 8 / Megabit,
        downloadTime, contentLength / downloadTime * 8 / Megabit);
}
</pre>

<h2>Execution</h2>

<p>Running the speed test application on a Windows 2012 Azure virtual machine, here's an average result:</p>

<pre>
%> ./SharePointOnlineSpeedTest.exe https://bugfree.sharepoint.com/sites/test/speedtest/testfile 1024
1024 MB uploaded in 280.9 seconds at 30.6 Mbit/s
1024 MB downloaded in 95.6 seconds at 89.8 Mbit/s
</pre>

<p>According
to <a href="https://support.office.com/en-au/article/SharePoint-Online-software-boundaries-and-limits-8f34ff47-b749-408b-abc0-b605e1f6d498">software
boundaries and limits for SharePoint Online</a>, the file attachment
size limit is 250MB and file upload limit is 2GB per file.</p>

<h2>Conclusion</h2>

<p>Uploading and downloading a document of one gigabyte is
surprisingly fast, considering that SharePoint must store the file in
an SQL Server content database (as opposed to blob storage build to
handle large files). In terms of available bandwidth, to some extend
we could use SharePoint Online document libraries as an alternative to
Azure blob storage. But SharePoint not being transactional will likely
cause issues down the road.</p>

</div>
