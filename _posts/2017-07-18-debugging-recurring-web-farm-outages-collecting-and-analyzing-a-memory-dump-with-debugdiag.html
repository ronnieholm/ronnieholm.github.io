---
layout: post
title: "Debugging recurring web farm outages: Collecting and analyzing a memory dump with DebugDiag"
date: 2017-07-18 12:00 UTC
---

<div id="post">

<p>
Part 1: <a href="/blog/2017/07/16/debugging-recurring-web-farm-outages-establishing-context-with-http-heartbeat-pings">Establishing context with HTTP heartbeat pings</a><br>
Part 2: <a href="/blog/2017/07/17/debugging-recurring-web-farm-outages-analyzing-windows-event-log-iis-log-sharepoint-log-with-powershell">Analyzing Windows Event Log, IIS log, and SharePoint log with PowerShell</a><br>
Part 3: Collecting and analyzing a memory dump with DebugDiag<br>
Part 4: Analyzing the memory dump with WinDbg (future)
</p>
  
<p>From
the <a href="/blog/2017/07/17/debugging-recurring-web-farm-outages-analyzing-windows-event-log-iis-log-sharepoint-log-with-powershell">previous</a>
post, it's clear that IIS is receiving requests and that ASP.NET
doesn't finish processing. To gain insights into what goes on inside
ASP.NET's request pipeline, this post outlines how to dump the w3wp
process' memory. Dumping at the right moment is crucial,
though. Ideally, we want to dump the moment ASP.NET flags itself as
unhealthy and right before w3wp recycles. We'll illustrate how to use
<a href="https://blogs.msdn.microsoft.com/debugdiag">DebugDiag</a> or
a timer do generate a dump file and analyze the dump using build-in
DebugDiag reports.</p>

<h4>DebugDiag dump collection</h4>

<p>DebugDiag may be use for generating a set of reports from the
dump. It's the capture on breakpoint hit part of DebugDiag that we're
interested in. Other tools, such as ProcDump doesn't support
generating on breakpoint. WinDbg does, but we had trouble getting it
to run on an old Windows Server 2008.</p>

<p>Once a dump has
been <a href="https://blogs.msdn.microsoft.com/webtopics/2016/04/05/data-collection-for-deadlock-detected-issue-of-a-iis-based-web-application">captured</a>
(when isapi!SSFReportUnhealthy is called by ASP.NET), the dump can be
moved to another machine for DebugDiag report generation and WinDbg
analysis. Unfortunately, DebugDiag requires running an MSI installer
on each server where we'd like to gather a dump. That may be an issue,
and ideally we prefer a non-intrusive capture mechanism.</p>

<h4>Scheduled dump collection</h4>

<p>With a small PowerShell script combined
with <a href="https://technet.microsoft.com/en-us/sysinternals/dd996900.aspx">ProcDump</a>,
we can create a scheduled dump mechanism. Of course, something similar
could be a achieved using a scheduled task, but the script approach is
about as non-intrusive as it gets:</p>

<pre>
$dumpAt = (Get-Date -Year 2017 -Month 7 -Day 17 -Hour 15 -Minute 39 -Second 00)
sleep -Seconds (New-TimeSpan -end $dumpAt).TotalSeconds
$pids = C:\Windows\system32\inetsrv\appcmd.exe list wps /apppool.name:"SP_Intranet_AppPool" /text:WP.NAME
$pids | foreach {
  .\procdump64 -accepteula -64 -ma $_ e:\
}  
</pre>

<p>A tricky part of the script is getting at the w3wp process
identifiers running under a specific application pool. With a server
hosting 16 IIS applications, a similar number of w3wp processes may
exist (more if running in a web garden setup). Another issue is that
during dump collection, the process gets suspended. Hence a dump must
be collected fast enough for IIS not to register w3wp as unresponsive
and recycle it. In general, the ProcDump -r switch clones a process,
resumes the original, and dumps the clone instead. But the -r switch
is unsupported with Windows Server 2008. Dump times for the current
case are in the range 10-50 seconds, writing a dump file in the 1-4 GB
range.</p>

<h4>Analyzing the dump DebugDiag reports</h4>

<p>Now we ask DebugDiag to generate all its report in a single
file. Inside report, we find a HttpContext Report. Based on the
HttpContext objects in the dump, that report lists a number of useful
information about each request (of which there're 304 in total). For
our case, the most interesting piece of information is the whether the
request is completes, and if not then for how long it's been running,
and which worker thread pool thread is responsible for the
request.</p>

<p>Here's a sample from the report listing a couple of completed
requests and every not completed ones. The heartbeat pings are the GET
requests to /org/it/pages/default.aspx:</p>

<pre>
HttpContext   Timeout   Completed   RunningSince   ThreadId   ReturnCode   Verb   RequestPath+QueryString 
1020c2550     110 Sec   Yes         ---                       200          GET    /favicon.ico  
105cc7988     360 Sec   Yes         ---                       200          GET    /_layouts/1030/styles/core.css rev=RlafelT4IyQwdwdVUY0P3w%3D%3D
112e66ee8     110 Sec   No          501 Sec        26         200          GET    /pages/frontpage.aspx
14da07450     110 Sec   No          661 Sec        27         200          GET    /org/it/pages/default.aspx
18f72bce8     110 Sec   No          246 Sec        52         200          GET    /pages/frontpage.aspx
18f7ab300     110 Sec   No           52 Sec        56         200          GET    /pages/frontpage.aspx 
1cb74f958     110 Sec   No          401 Sec        21         200          GET    /org/it/pages/default.aspx
1cb7e31b8     110 Sec   No          348 Sec        28         200          GET    /pages/frontpage.aspx   
1cb8588a8     110 Sec   No          204 Sec        54         200          GET    /pages/frontpage.aspx
1cb86b8d8     110 Sec   No          109 Sec        55         200          GET    /org/it/pages/default.aspx
</pre>

<p>The above listing is from a dump collected at 23:00. The report
lists 14% threads blocked (8 threads in total). A later dump collected
at 23:15, but on another day, reports 30% threads blocked (22 threads
in total). While the heartbeat script may cause the number of blocked
thread to increase faster than if only regular users had accessed the
site, the trend seems clear.</p>

<p>At neither 23:00 nor 23:15 has ASP.NET raised the unhealthy
flag. Only heartbeat GET requests report timeouts and Internet
Explorer shows blank pages. In the later dump, however, the maximum
RunningSince across all threads have increased to 1,747
seconds. Subtracting 1,747 seconds from 23:15 indicates the request
was received by IIS around 22:46, just about when the heartbeat script
started detecting downtime.</p>

<p>Matching these not completed requests, the report has another
section showing eight undisposed SPWeb objects and eight undisposed
SPRequest objects. The thread IDs of each of these match up with the
block requests so it's safe to assume that these undisposed resources
are currently in use, i.e., they don't represent a memory leak. The
addresses of each come in handy once we load the dump into WinDbg.</p>

<pre>
Address            SPRequest   Auto-Dispose   Auth     Thread   Url [Title]
0000000112e6d400   Own         Thread         (null)   26       (null) []
000000014ddda9a8   Own         Thread         (null)   27       (null) []
000000018f735380   Own         Thread         (null)   52       (null) []
000000018f7b4690   Own         Thread         (null)   56       (null) []
00000001cb793a90   Own         Thread         (null)   21       (null) []
00000001cb7e89c0   Own         Thread         (null)   28       (null) []
00000001cb85f020   Own         Thread         (null)   54       (null) []
00000001cb8b02e0   Own         Thread         (null)   55       (null) []

SPContext          Thread   Linked Objects
0000000112e6d7a0   26	    SPSWeb:0000000112e6d400 
000000014dddad48   27	    SPSWeb:000000014ddda9a8 
000000018f735720   52	    SPSWeb:000000018f735380 
000000018f7b4a30   56	    SPSWeb:000000018f7b4690 
00000001cb793e30   21	    SPSWeb:00000001cb793a90 
00000001cb7e8d60   28	    SPSWeb:00000001cb7e89c0 
00000001cb85f3c0   54	    SPSWeb:00000001cb85f020 
00000001cb8b0680   55	    SPSWeb:00000001cb8b02e0 
</pre>

<p>Finally, the Client Connections section details currently executing
requests. Here's one of the heartbeat requests, including its NTLM
authentication headers mentioned in the previous post. For references,
172.27.18.210 is my reference laptop running the heartbeat tool and
172.27.18.8 corresponds to Web front-end server 2:</p>

<pre>
Client connection from 172.27.18.210:58666 to 172.27.18.8:80 
Host Header            intranet.acme.dk:80 
GET request for        /org/it/pages/default.aspx
HTTP Version           HTTP/1.1 
SSL Request            False 
Time alive             0-2:0-57:0-56 
QueryString    
Request mapped to    
HTTP Request State     HTR_READING_CLIENT_REQUEST 
Native Request State   NREQ_STATE_PROCESS 
HTTP Headers   Authorization: NTLM TlRMTVNTUA...AAAAAAAAAAAAAAAAAA==
               Host: intranet.acme.dk
</pre>

<h4>Conclusion</h4>

<p>DebugDiag reports confirm our suspicion that ASP.NET worker thread
aren't processing requests, and that the longer we wait the more
critical the issue becomes. Eventually w3wp signals WAS that it's
unhealthy and in a deadlocked state, and gets restarted. This process
repeats itself a number of times until the issue resolve itself.</p>

<p>Still unanswered is on what the worker threads block. DebugDiag
does provide stack traces of each blocked thread listing functions
only, but not arguments. We didn't include the stack trace above
because with the thread identifiers, WinDbg is better suited for
digging into that side.</p>

<p>In the next post, we'll load the DebugDiag generated DMP file into
WinDbg and learn on what the worker thread pool threads is
blocked.</p>

</div>
